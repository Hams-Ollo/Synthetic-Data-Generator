#!/usr/bin/env python3
"""
Production Batch Generator for Golden Incidents v2.0
===================================================

Production-ready script for generating large batches of synthetic
ServiceNow incident data with monitoring, checkpointing, and recovery.

Features:
- Large-scale batch processing with checkpointing
- Progress monitoring and status reporting
- Automatic recovery from failures
- Performance optimization
- Resource usage monitoring
- Email notifications (optional)
- Integration with Azure Monitor

Author: Hans Havlik
Date: May 30, 2025
Version: 1.0.0
"""

import json
import os
import sys
import time
import logging
import argparse
import pickle
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import signal

# Add path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'Config'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'Main'))

try:
    from golden_incident_generator_v2 import GoldenIncidentGeneratorV2, IncidentMetrics
except ImportError as e:
    print(f"Failed to import Golden Incident Generator: {e}")
    sys.exit(1)


class ProductionBatchConfig:
    """Configuration for production batch processing"""
    
    def __init__(self, config_file: Optional[str] = None):
        self.config = self.load_config(config_file)
        
    def load_config(self, config_file: Optional[str]) -> Dict[str, Any]:
        """Load production batch configuration"""
        default_config = {
            "batch_settings": {
                "target_incidents": 1000,
                "batch_size": 25,
                "max_concurrent_batches": 3,
                "checkpoint_interval": 100,
                "recovery_enabled": True,
                "auto_export": True
            },
            "performance_settings": {
                "max_memory_mb": 2048,
                "max_cpu_percent": 80,
                "cooling_period_seconds": 30,
                "resource_check_interval": 60
            },
            "monitoring_settings": {
                "enable_progress_reporting": True,
                "status_update_interval": 30,
                "enable_metrics_collection": True,
                "log_level": "INFO"
            },
            "export_settings": {
                "formats": ["csv", "json", "excel"],
                "include_metadata": True,
                "compression": True,
                "split_large_files": True,
                "max_records_per_file": 5000
            },
            "notification_settings": {
                "enable_email": False,
                "email_recipients": [],
                "smtp_server": "",
                "smtp_port": 587,
                "smtp_username": "",
                "smtp_password": ""
            },
            "azure_monitor": {
                "enable_telemetry": False,
                "workspace_id": "",
                "shared_key": "",
                "custom_log_name": "GoldenIncidentGenerator"
            }
        }
        
        if config_file and Path(config_file).exists():
            try:
                with open(config_file, 'r') as f:
                    file_config = json.load(f)
                # Merge configurations
                for key, value in file_config.items():
                    if isinstance(value, dict) and key in default_config:
                        default_config[key].update(value)
                    else:
                        default_config[key] = value
            except Exception as e:
                logging.warning(f"Failed to load config file {config_file}: {e}")
                
        return default_config


class ProductionBatchMonitor:
    """Monitor for production batch processing"""
    
    def __init__(self, config: ProductionBatchConfig):
        self.config = config
        self.start_time = datetime.now()
        self.last_checkpoint = datetime.now()
        self.metrics_history = []
        self.is_monitoring = False
        self.monitor_thread = None
        
    def start_monitoring(self):
        """Start background monitoring"""
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """Stop background monitoring"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
            
    def _monitoring_loop(self):
        """Background monitoring loop"""
        while self.is_monitoring:
            try:
                # Collect system metrics
                cpu_percent = psutil.cpu_percent(interval=1)
                memory_info = psutil.virtual_memory()
                disk_info = psutil.disk_usage('/')
                
                metrics = {
                    'timestamp': datetime.now().isoformat(),
                    'cpu_percent': cpu_percent,
                    'memory_percent': memory_info.percent,
                    'memory_mb': memory_info.used / 1024 / 1024,
                    'disk_percent': disk_info.percent,
                    'process_count': len(psutil.pids())
                }
                
                self.metrics_history.append(metrics)
                
                # Keep only last hour of metrics
                cutoff_time = datetime.now() - timedelta(hours=1)
                self.metrics_history = [
                    m for m in self.metrics_history 
                    if datetime.fromisoformat(m['timestamp']) > cutoff_time
                ]
                
                # Check resource limits
                if cpu_percent > self.config.config['performance_settings']['max_cpu_percent']:
                    logging.warning(f"High CPU usage detected: {cpu_percent}%")
                    
                if memory_info.used / 1024 / 1024 > self.config.config['performance_settings']['max_memory_mb']:
                    logging.warning(f"High memory usage detected: {memory_info.used / 1024 / 1024:.1f} MB")
                
                time.sleep(self.config.config['monitoring_settings']['status_update_interval'])
                
            except Exception as e:
                logging.error(f"Monitoring error: {e}")
                time.sleep(5)
                
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary"""
        if not self.metrics_history:
            return {}
            
        recent_metrics = self.metrics_history[-10:]  # Last 10 readings
        
        return {
            'avg_cpu_percent': sum(m['cpu_percent'] for m in recent_metrics) / len(recent_metrics),
            'avg_memory_percent': sum(m['memory_percent'] for m in recent_metrics) / len(recent_metrics),
            'avg_memory_mb': sum(m['memory_mb'] for m in recent_metrics) / len(recent_metrics),
            'peak_cpu_percent': max(m['cpu_percent'] for m in recent_metrics),
            'peak_memory_mb': max(m['memory_mb'] for m in recent_metrics),
            'total_runtime': str(datetime.now() - self.start_time)
        }


class ProductionBatchCheckpoint:
    """Checkpoint management for production batch processing"""
    
    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        
    def save_checkpoint(self, 
                       session_id: str, 
                       progress: Dict[str, Any], 
                       generated_incidents: List[Any],
                       metrics: IncidentMetrics):
        """Save checkpoint data"""
        checkpoint_data = {
            'session_id': session_id,
            'timestamp': datetime.now().isoformat(),
            'progress': progress,
            'metrics': metrics.__dict__,
            'incident_count': len(generated_incidents)
        }
        
        # Save checkpoint metadata
        checkpoint_file = self.checkpoint_dir / f"{session_id}_checkpoint.json"
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint_data, f, indent=2, default=str)
            
        # Save incidents data (pickled for performance)
        incidents_file = self.checkpoint_dir / f"{session_id}_incidents.pkl"
        with open(incidents_file, 'wb') as f:
            pickle.dump(generated_incidents, f)
            
        logging.info(f"Checkpoint saved: {checkpoint_file}")
        
    def load_checkpoint(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Load checkpoint data"""
        checkpoint_file = self.checkpoint_dir / f"{session_id}_checkpoint.json"
        incidents_file = self.checkpoint_dir / f"{session_id}_incidents.pkl"
        
        if not checkpoint_file.exists() or not incidents_file.exists():
            return None
            
        try:
            # Load checkpoint metadata
            with open(checkpoint_file, 'r') as f:
                checkpoint_data = json.load(f)
                
            # Load incidents data
            with open(incidents_file, 'rb') as f:
                incidents = pickle.load(f)
                
            checkpoint_data['incidents'] = incidents
            logging.info(f"Checkpoint loaded: {checkpoint_file}")
            return checkpoint_data
            
        except Exception as e:
            logging.error(f"Failed to load checkpoint: {e}")
            return None
            
    def cleanup_checkpoint(self, session_id: str):
        """Clean up checkpoint files"""
        checkpoint_file = self.checkpoint_dir / f"{session_id}_checkpoint.json"
        incidents_file = self.checkpoint_dir / f"{session_id}_incidents.pkl"
        
        for file_path in [checkpoint_file, incidents_file]:
            if file_path.exists():
                try:
                    file_path.unlink()
                    logging.info(f"Cleaned up checkpoint file: {file_path}")
                except Exception as e:
                    logging.warning(f"Failed to cleanup {file_path}: {e}")


class ProductionBatchGenerator:
    """Production-ready batch generator with monitoring and recovery"""
    
    def __init__(self, config_file: Optional[str] = None):
        self.config = ProductionBatchConfig(config_file)
        self.monitor = ProductionBatchMonitor(self.config)
        self.checkpoint = ProductionBatchCheckpoint()
        self.session_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.generator = None
        self.stop_requested = False
        
        # Setup signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.setup_logging()
        
    def setup_logging(self):
        """Setup production logging"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"production_batch_{self.session_id}.log"
        
        logging.basicConfig(
            level=getattr(logging, self.config.config['monitoring_settings']['log_level']),
            format='%(asctime)s - %(name)s - %(levelname)s - [%(process)d] - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ],
            force=True
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Production batch session started: {self.session_id}")
        
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully"""
        self.logger.info(f"Received signal {signum}, initiating graceful shutdown...")
        self.stop_requested = True
        
    def initialize_generator(self):
        """Initialize the incident generator with config path and batch settings"""
        try:
            # Pass config file path and batch settings to GoldenIncidentGeneratorV2
            config_path = None
            if hasattr(self.config, 'config') and 'config_file' in self.config.__dict__:
                config_path = self.config.config_file
            # Use config file path if provided, else default
            self.generator = GoldenIncidentGeneratorV2(
                config_path=config_path,
                batch_size=self.config.config['batch_settings']['batch_size'],
                max_retries=self.config.config['batch_settings'].get('max_retries', 3),
                retry_delay=self.config.config['batch_settings'].get('retry_delay', 1.0)
            )
            self.logger.info("Golden Incident Generator initialized successfully")
            return True
        except Exception as e:
            self.logger.error(f"Failed to initialize generator: {e}")
            return False
            
    def check_system_resources(self) -> bool:
        """Check if system resources are within limits"""
        cpu_percent = psutil.cpu_percent()
        memory_info = psutil.virtual_memory()
        
        max_cpu = self.config.config['performance_settings']['max_cpu_percent']
        max_memory_mb = self.config.config['performance_settings']['max_memory_mb']
        
        if cpu_percent > max_cpu:
            self.logger.warning(f"CPU usage too high: {cpu_percent}% > {max_cpu}%")
            return False
            
        if memory_info.used / 1024 / 1024 > max_memory_mb:
            self.logger.warning(f"Memory usage too high: {memory_info.used / 1024 / 1024:.1f} MB > {max_memory_mb} MB")
            return False
            
        return True
        
    def generate_production_batch(self, resume_session: Optional[str] = None) -> bool:
        """Generate large batch with checkpointing and monitoring"""
        
        # Initialize generator
        if not self.initialize_generator():
            return False
            
        # Start monitoring
        self.monitor.start_monitoring()
        
        try:
            # Check for resume
            checkpoint_data = None
            if resume_session:
                checkpoint_data = self.checkpoint.load_checkpoint(resume_session)
                if checkpoint_data:
                    self.logger.info(f"Resuming session: {resume_session}")
                    self.session_id = resume_session
                    
            # Initialize progress
            target_incidents = self.config.config['batch_settings']['target_incidents']
            batch_size = self.config.config['batch_settings']['batch_size']
            checkpoint_interval = self.config.config['batch_settings']['checkpoint_interval']
            
            if checkpoint_data:
                generated_incidents = checkpoint_data['incidents']
                completed = len(generated_incidents)
                self.generator.generated_incidents = generated_incidents
                self.logger.info(f"Resuming from {completed} incidents")
            else:
                generated_incidents = []
                completed = 0
                
            # Main generation loop
            while completed < target_incidents and not self.stop_requested:
                
                # Check system resources
                if not self.check_system_resources():
                    cooling_period = self.config.config['performance_settings']['cooling_period_seconds']
                    self.logger.info(f"Resource limits exceeded, cooling down for {cooling_period}s")
                    time.sleep(cooling_period)
                    continue
                    
                # Calculate batch size for this iteration
                remaining = target_incidents - completed
                current_batch_size = min(batch_size, remaining)
                
                self.logger.info(f"Generating batch: {current_batch_size} incidents ({completed}/{target_incidents} completed)")
                
                # Generate batch
                batch_start_time = time.time()
                new_incidents = self.generator.generate_batch(current_batch_size)
                batch_time = time.time() - batch_start_time
                
                # Update progress
                completed += len(new_incidents)
                generated_incidents.extend(new_incidents)
                
                self.logger.info(f"Batch completed: {len(new_incidents)} incidents in {batch_time:.2f}s")
                
                # Checkpoint if needed
                if completed % checkpoint_interval == 0 or completed >= target_incidents:
                    progress = {
                        'completed': completed,
                        'target': target_incidents,
                        'success_rate': self.generator.metrics.success_rate,
                        'last_update': datetime.now().isoformat()
                    }
                    
                    self.checkpoint.save_checkpoint(
                        self.session_id,
                        progress,
                        generated_incidents,
                        self.generator.metrics
                    )
                    
                # Progress reporting
                percentage = (completed / target_incidents) * 100
                eta_seconds = (batch_time * (target_incidents - completed)) / current_batch_size if current_batch_size > 0 else 0
                eta = datetime.now() + timedelta(seconds=eta_seconds)
                
                self.logger.info(f"Progress: {completed}/{target_incidents} ({percentage:.1f}%) - ETA: {eta.strftime('%Y-%m-%d %H:%M:%S')}")
                
            # Final processing
            if self.stop_requested:
                self.logger.warning("Generation stopped by user request")
            else:
                self.logger.info(f"Generation completed: {completed} incidents generated")
                
            # Export results
            if self.config.config['batch_settings']['auto_export'] and generated_incidents:            self.export_results(generated_incidents)
                
            # Cleanup checkpoint if completed successfully
            if completed >= target_incidents:
                self.checkpoint.cleanup_checkpoint(self.session_id)
                
            return True
            
        except Exception as e:
            self.logger.error(f"Production batch generation failed: {e}")
            return False
            
        finally:
            self.monitor.stop_monitoring()
            
    def export_results(self, incidents: List[Any]):
        """Export results in configured formats using updated generator methods"""
        from pathlib import Path
        output_dir = Path("synthetic_data_output")
        output_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_filename = f"production_incidents_{self.session_id}_{timestamp}"
        formats = self.config.config['export_settings']['formats']
        include_metadata = self.config.config['export_settings']['include_metadata']
        max_records = self.config.config['export_settings']['max_records_per_file']
        # Ensure both Excel and JSON are always included
        if 'excel' not in formats:
            formats.append('excel')
        if 'json' not in formats:
            formats.append('json')
        # Update generator's incidents for export
        self.generator.generated_incidents = incidents
        try:
            if 'csv' in formats:
                if len(incidents) > max_records:
                    for i in range(0, len(incidents), max_records):
                        chunk = incidents[i:i + max_records]
                        self.generator.generated_incidents = chunk
                        filename = output_dir / f"{base_filename}_part{i//max_records + 1}.csv"
                        self.generator.export_to_csv(str(filename), include_metadata)
                else:
                    filename = output_dir / f"{base_filename}.csv"
                    self.generator.export_to_csv(str(filename), include_metadata)
            if 'json' in formats:
                self.generator.generated_incidents = incidents
                filename = output_dir / f"{base_filename}.json"
                self.generator.export_to_json(str(filename), include_metadata)
            if 'excel' in formats:
                self.generator.generated_incidents = incidents
                filename = output_dir / f"{base_filename}.xlsx"
                self.generator.export_to_excel(str(filename), True)
            self.logger.info(f"Results exported in {len(formats)} format(s) to synthetic_data_output folder")
        except Exception as e:
            self.logger.error(f"Export failed: {e}")

    def print_final_summary(self):
        """Print comprehensive final summary using updated metrics fields"""
        performance = self.monitor.get_performance_summary()
        
        print("\n" + "="*80)
        print("PRODUCTION BATCH GENERATION - FINAL SUMMARY")
        print("="*80)
        
        print(f"\nSESSION INFORMATION:")
        print(f"  Session ID: {self.session_id}")
        print(f"  Start Time: {self.monitor.start_time}")
        print(f"  End Time: {datetime.now()}")
        print(f"  Total Runtime: {performance.get('total_runtime', 'N/A')}")
        
        if self.generator:
            print(f"\nGENERATION STATISTICS:")
            print(f"  Total Generated: {self.generator.metrics.total_generated}")
            print(f"  Total Failed: {self.generator.metrics.total_failed}")
            print(f"  Success Rate: {self.generator.metrics.success_rate:.1f}%")
            print(f"  Average Generation Time: {self.generator.metrics.avg_generation_time:.2f}s")
            print(f"  Total Tokens Used: {self.generator.metrics.total_tokens_used:,}")
            print(f"  Estimated Cost: ${self.generator.metrics.cost_estimate:.4f}")
            
        print(f"\nPERFORMANCE METRICS:")
        print(f"  Average CPU Usage: {performance.get('avg_cpu_percent', 0):.1f}%")
        print(f"  Peak CPU Usage: {performance.get('peak_cpu_percent', 0):.1f}%")
        print(f"  Average Memory Usage: {performance.get('avg_memory_mb', 0):.1f} MB")
        print(f"  Peak Memory Usage: {performance.get('peak_memory_mb', 0):.1f} MB")
        
        print("\n" + "="*80)


def main():
    """Main function for production batch processing"""
    parser = argparse.ArgumentParser(
        description="Production Batch Generator for Golden Incidents v2.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generate 1000 incidents with default settings
  python production_batch_generator.py --target 1000
  
  # Generate with custom configuration
  python production_batch_generator.py --target 5000 --config production_config.json
  
  # Resume interrupted session
  python production_batch_generator.py --resume batch_20250530_143022
  
  # Generate with custom batch size and checkpointing
  python production_batch_generator.py --target 2000 --batch-size 100 --checkpoint-interval 200
        """
    )
    
    # Generation arguments
    parser.add_argument('--target', type=int, default=1000,
                       help='Target number of incidents to generate')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='Batch size for generation')
    parser.add_argument('--checkpoint-interval', type=int, default=100,
                       help='Checkpoint every N incidents')
    
    # Configuration arguments
    parser.add_argument('--config', type=str,
                       help='Production configuration file')
    parser.add_argument('--resume', type=str,
                       help='Resume from checkpoint session ID')
    
    # Performance arguments
    parser.add_argument('--max-cpu', type=int, default=80,
                       help='Maximum CPU usage percentage')
    parser.add_argument('--max-memory', type=int, default=2048,
                       help='Maximum memory usage in MB')
    
    # Export arguments
    parser.add_argument('--no-auto-export', action='store_true',
                       help='Disable automatic export')
    parser.add_argument('--export-formats', nargs='+', 
                       choices=['csv', 'json', 'excel'], default=['csv', 'json'],
                       help='Export formats')
    
    # Monitoring arguments
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='Logging level')
    
    args = parser.parse_args()
    
    try:
        # Create production batch generator
        generator = ProductionBatchGenerator(args.config)
        
        # Override config with command line arguments
        generator.config.config['batch_settings']['target_incidents'] = args.target
        generator.config.config['batch_settings']['batch_size'] = args.batch_size
        generator.config.config['batch_settings']['checkpoint_interval'] = args.checkpoint_interval
        generator.config.config['batch_settings']['auto_export'] = not args.no_auto_export
        generator.config.config['export_settings']['formats'] = args.export_formats
        generator.config.config['performance_settings']['max_cpu_percent'] = args.max_cpu
        generator.config.config['performance_settings']['max_memory_mb'] = args.max_memory
        generator.config.config['monitoring_settings']['log_level'] = args.log_level
        
        print(f"Starting production batch generation...")
        print(f"Target: {args.target} incidents")
        print(f"Batch Size: {args.batch_size}")
        print(f"Session ID: {generator.session_id}")
        
        # Run generation
        success = generator.generate_production_batch(args.resume)
        
        # Print final summary
        generator.print_final_summary()
        
        if success:
            print("\nüéâ Production batch generation completed successfully!")
            return 0
        else:
            print("\n‚ùå Production batch generation failed!")
            return 1
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Production batch generation interrupted by user.")
        return 1
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
